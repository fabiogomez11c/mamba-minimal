{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import time\n",
    "\n",
    "from typing import Union\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = \"auto\"\n",
    "    d_conv: int = 4\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = \"auto\"\n",
    "    d_conv: int = 4\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == \"auto\":\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
    "            self.vocab_size += (\n",
    "                self.pad_vocab_size_multiple\n",
    "                - self.vocab_size % self.pad_vocab_size_multiple\n",
    "            )\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "        self.lm_head.weight = (\n",
    "            self.embedding.weight\n",
    "        )  # Tie output projection to embedding weights.\n",
    "        # See \"Weight Tying\" paper\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "\n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "\n",
    "        \"\"\"\n",
    "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "\n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(\n",
    "                model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False\n",
    "            )\n",
    "            return json.load(open(resolved_archive_file))\n",
    "\n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(\n",
    "                model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False\n",
    "            )\n",
    "            return torch.load(\n",
    "                resolved_archive_file, weights_only=True, map_location=\"cpu\"\n",
    "            )\n",
    "\n",
    "        config_data = load_config_hf(pretrained_model_name)\n",
    "        args = ModelArgs(\n",
    "            d_model=config_data[\"d_model\"],\n",
    "            n_layer=config_data[\"n_layer\"],\n",
    "            vocab_size=config_data[\"vocab_size\"],\n",
    "        )\n",
    "        model = Mamba(args)\n",
    "\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            new_key = key.replace(\"backbone.\", \"\")\n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = RMSNorm(args.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "\n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x)) + x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(\n",
    "            args.d_inner, args.dt_rank + args.d_state * 2, bias=False\n",
    "        )\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), \"n -> d n\", d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(\n",
    "            split_size=[self.args.d_inner, self.args.d_inner], dim=-1\n",
    "        )\n",
    "\n",
    "        x = rearrange(x, \"b l d_in -> b d_in l\")\n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, \"b d_in l -> b l d_in\")\n",
    "\n",
    "        x = F.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "\n",
    "        y = y * F.silu(res)\n",
    "\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(\n",
    "            split_size=[self.args.dt_rank, n, n], dim=-1\n",
    "        )  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(\n",
    "            x, delta, A, B, C, D\n",
    "        )  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, \"b l d_in, d_in n -> b l d_in n\"))\n",
    "        deltaB_u = einsum(delta, B, u, \"b l d_in, b l n, b l d_in -> b l d_in n\")\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], \"b d_in n, b n -> b d_in\")\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (\n",
    "            x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "        )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 200/200 [00:00<00:00, 341kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 5.49G/5.49G [02:27<00:00, 37.2MB/s]\n",
      "/Users/fabio/miniconda3/envs/minmamba/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "tokenizer_config.json: 100%|██████████| 156/156 [00:00<00:00, 60.5kB/s]\n",
      "vocab.json: 100%|██████████| 1.08M/1.08M [00:00<00:00, 5.38MB/s]\n",
      "merges.txt: 100%|██████████| 457k/457k [00:00<00:00, 3.36MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 10.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 251kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"state-spaces/mamba-1.4b\"\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (embedding): Embedding(50280, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-47): 48 x ResidualBlock(\n",
       "      (mixer): MambaBlock(\n",
       "        (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
       "        (x_proj): Linear(in_features=4096, out_features=160, bias=False)\n",
       "        (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
       "        (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): RMSNorm()\n",
       "  (lm_head): Linear(in_features=2048, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    n_tokens_to_gen: int = 50,\n",
    "    sample: bool = True,\n",
    "    top_k: int = 40,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    times = []\n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        tic = time.time()\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            indices_to_input = indices_to_input.to(device)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "            next_token_logits = next_token_logits.to(\"cpu\")\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "\n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "        times.append(time.time() - tic)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\n",
    "    return output_completions, torch.mean(torch.Tensor(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg, t = generate(model, tokenizer, \"The meaning of life is \", n_tokens_to_gen=100)\n",
    "print(msg)\n",
    "print(f\"\\nTime per token: {t:.2f} token/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of params: {pytorch_total_params / 1000000000:.2f}B\")\n",
    "first_param = next(model.parameters())\n",
    "print(f\"Precision used: {first_param.dtype}\")\n",
    "print(f\"Estimated model size: {pytorch_total_params * 32 / 8 * 1e-9:.2f} GB\")\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minmamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
